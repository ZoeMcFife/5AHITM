{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZoeMcFife/5AHITM/blob/main/INSY/Colab/%232025-03-06%23/KNN-Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIibjihmHJYC"
      },
      "source": [
        "# Assignment: Voter classification using exit poll data\n",
        "\n",
        "*This is based on the course of [Fraida Fund](https://colab.research.google.com/github/ffund/ml-notebooks/blob/master/notebooks/1-colab-tour.ipynb) for  NYU Tandon School of Engineering*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yboQG59HJYE"
      },
      "source": [
        "**TODO**: Edit this cell to fill in your name:\n",
        "\n",
        "-   **Name**: Bunea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiQUp30HHJYF"
      },
      "source": [
        "In this notebook, we will explore the problem of voter classification.\n",
        "\n",
        "Given demographic data about a voter and their opinions on certain key issues, can we predict their vote in the 2016 U.S. presidential election? We will attempt this using a K nearest neighbor classifier.\n",
        "\n",
        "In the first few sections of this notebook, I will show you how to prepare the data and and use a K nearest neighbors classifier for this task, including:\n",
        "\n",
        "-   getting the data and loading it into the workspace.\n",
        "-   preparing the data: dealing with missing data, encoding categorical data in numeric format, and splitting into training and test.\n",
        "\n",
        "In the last few sections of the notebook, you will have to improve the basic model for better performance, using a custom distance metric and using feature selection or feature weighting. In these sections, you will have specific criteria to satisfy for each task.\n",
        "\n",
        "**However, you should also make sure your overall solution is good!** An excellent solution to this problem will achieve greater than 80% validation accuracy. A great solution will achieve 75% or higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ga5tMWUHJYI"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p8zNiZ41HJYK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import ShuffleSplit, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mZrQKSRHJYR"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6JPcL4EHJYU"
      },
      "source": [
        "The data for this notebook comes from the [U.S. National Election Day Exit Polls](https://ropercenter.cornell.edu/exit-polls/us-national-election-day-exit-polls).\n",
        "\n",
        "Here’s a brief description of how exit polls work.\n",
        "\n",
        "Exit polls are conducted by Edison Research on behalf of a consortium of media organizations.\n",
        "\n",
        "First, the member organizations decide what races to cover, what sample size they want, what questions should be asks, and other details. Then, sample precincts are selected, and local interviewers are hired and trained. Then, at those precincts, the local interviewer approaches a subset of voters as they exit the polls (for example, every third voter, or every fifth voter, depending on the required sample size).\n",
        "\n",
        "When a voter is approached, they are asked if they are willing to fill out a questionnaire. Typically about 40-50% agree. (For those that decline, the interviewer visually estimates their age, race, and gender, and notes this information, so that the response rate by demographic is known and responses can be weighted accordingly in order to be more representative of the population.)\n",
        "\n",
        "Voters that agree to participate are then given an form with 15-20 questions. They fill in the form (anonymously), fold it, and put it in a small ballot box.\n",
        "\n",
        "Three times during the day, the interviewers will stop, take the questionnaires, compile the results, and call them in to the Edison Research phone center. The results are reported immediately to the media organizations that are consortium members.\n",
        "\n",
        "In addition to the poll of in-person voters, absentee and early voters (who are not at the polls on Election Day) are surveyed by telephone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d6roO6qHJYX"
      },
      "source": [
        "### Download the data and documentation\n",
        "\n",
        "The exit poll data is not freely available on the web, but you can download the dataset on Moodle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV8Xg4DQHJYg"
      },
      "source": [
        "### Upload into Colab filesystem\n",
        "\n",
        "To get the data into Colab, run the following cell. Upload the CSV file you just downloaded (`31116396_National2016.csv`) to your Colab workspace. Wait until the uploaded has **completely** finished - it may take a while, depending on the quality of your network connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n4ue4ULkHJYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "7dfac9c4-7a67-4026-aeca-6d4852315396"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-053800a0-c44d-415f-93b9-0fff9e0888e5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-053800a0-c44d-415f-93b9-0fff9e0888e5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 31116396_National2016.csv to 31116396_National2016.csv\n",
            "User uploaded file \"31116396_National2016.csv\" with length 26258607 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkkl9QZPHJYj"
      },
      "source": [
        "### Load data with pandas\n",
        "\n",
        "Now, use the `read_csv` function in `pandas` to read in the file.\n",
        "\n",
        "Also use `head` to view the first few rows of data and make sure that everything is read in correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YJjlVcnqHJYk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "91298dc6-8687-45db-96fd-7f04c65d81a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d2daf1675d09>:1: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('31116396_National2016.csv')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       ID             PRES                       HOU    WEIGHT @2WAYPRES16  \\\n",
              "0  135355  Hillary Clinton  The Democratic candidate  6.530935               \n",
              "1  135356  Hillary Clinton  The Democratic candidate  6.479016               \n",
              "2  135357  Hillary Clinton  The Democratic candidate  8.493230               \n",
              "3  135358  Hillary Clinton  The Democratic candidate  3.761814               \n",
              "4  135359  Hillary Clinton  The Democratic candidate  3.470473               \n",
              "\n",
              "     AGE   AGE3   AGE8  AGE45  AGE49  ... TRUMPWOMEN TRUMPWOMENB UNIONHH12  \\\n",
              "0  18-29  18-29  18-24  18-44  18-49  ...                                    \n",
              "1  18-29  18-29  25-29  18-44  18-49  ...                                    \n",
              "2  30-44  30-59  30-39  18-44  18-49  ...                                    \n",
              "3  30-44  30-59  30-39  18-44  18-49  ...                                    \n",
              "4  45-65  30-59  45-49    45+  18-49  ...                                    \n",
              "\n",
              "     VERSION VETVOTER WHITEREL WHNCLINC WHTEVANG WPROTBRN WPROTBRN3  \n",
              "0  Version 1                         No                              \n",
              "1  Version 1                         No                              \n",
              "2  Version 1                         No                              \n",
              "3  Version 1                         No                              \n",
              "4  Version 1                         No                              \n",
              "\n",
              "[5 rows x 138 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92c36bfb-4873-4d64-8dd8-4ca1fd9e5688\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>PRES</th>\n",
              "      <th>HOU</th>\n",
              "      <th>WEIGHT</th>\n",
              "      <th>@2WAYPRES16</th>\n",
              "      <th>AGE</th>\n",
              "      <th>AGE3</th>\n",
              "      <th>AGE8</th>\n",
              "      <th>AGE45</th>\n",
              "      <th>AGE49</th>\n",
              "      <th>...</th>\n",
              "      <th>TRUMPWOMEN</th>\n",
              "      <th>TRUMPWOMENB</th>\n",
              "      <th>UNIONHH12</th>\n",
              "      <th>VERSION</th>\n",
              "      <th>VETVOTER</th>\n",
              "      <th>WHITEREL</th>\n",
              "      <th>WHNCLINC</th>\n",
              "      <th>WHTEVANG</th>\n",
              "      <th>WPROTBRN</th>\n",
              "      <th>WPROTBRN3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135355</td>\n",
              "      <td>Hillary Clinton</td>\n",
              "      <td>The Democratic candidate</td>\n",
              "      <td>6.530935</td>\n",
              "      <td></td>\n",
              "      <td>18-29</td>\n",
              "      <td>18-29</td>\n",
              "      <td>18-24</td>\n",
              "      <td>18-44</td>\n",
              "      <td>18-49</td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Version 1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>135356</td>\n",
              "      <td>Hillary Clinton</td>\n",
              "      <td>The Democratic candidate</td>\n",
              "      <td>6.479016</td>\n",
              "      <td></td>\n",
              "      <td>18-29</td>\n",
              "      <td>18-29</td>\n",
              "      <td>25-29</td>\n",
              "      <td>18-44</td>\n",
              "      <td>18-49</td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Version 1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>135357</td>\n",
              "      <td>Hillary Clinton</td>\n",
              "      <td>The Democratic candidate</td>\n",
              "      <td>8.493230</td>\n",
              "      <td></td>\n",
              "      <td>30-44</td>\n",
              "      <td>30-59</td>\n",
              "      <td>30-39</td>\n",
              "      <td>18-44</td>\n",
              "      <td>18-49</td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Version 1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>135358</td>\n",
              "      <td>Hillary Clinton</td>\n",
              "      <td>The Democratic candidate</td>\n",
              "      <td>3.761814</td>\n",
              "      <td></td>\n",
              "      <td>30-44</td>\n",
              "      <td>30-59</td>\n",
              "      <td>30-39</td>\n",
              "      <td>18-44</td>\n",
              "      <td>18-49</td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Version 1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>135359</td>\n",
              "      <td>Hillary Clinton</td>\n",
              "      <td>The Democratic candidate</td>\n",
              "      <td>3.470473</td>\n",
              "      <td></td>\n",
              "      <td>45-65</td>\n",
              "      <td>30-59</td>\n",
              "      <td>45-49</td>\n",
              "      <td>45+</td>\n",
              "      <td>18-49</td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Version 1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>No</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 138 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92c36bfb-4873-4d64-8dd8-4ca1fd9e5688')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-92c36bfb-4873-4d64-8dd8-4ca1fd9e5688 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-92c36bfb-4873-4d64-8dd8-4ca1fd9e5688');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23fd98d9-2996-4562-8c65-fe5c05dc8370\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23fd98d9-2996-4562-8c65-fe5c05dc8370')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23fd98d9-2996-4562-8c65-fe5c05dc8370 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv('31116396_National2016.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2eE4sWZHJYl"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-KEiioDHJYm"
      },
      "source": [
        "Survey data can be tricky to work with, because surveys often “branch”; the questions that are asked depends on a respondent’s answers to other questions.\n",
        "\n",
        "In this case, different respondents fill out different versions of the survey. Review pages 7-11 of the “Study Documentation, Questionnaire, and Codebooks” PDF file you downloaded earlier, which shows the five different questionnaire versions used for the 2016 exit polls.\n",
        "\n",
        "Note that in a red box next to each question, you can see the name of the variable (column name) that the respondent’s answer will be stored in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_TVH_wjHJYo"
      },
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/ffund/ml-notebooks/master/notebooks/images/exit-poll-survey-versions.png\" alt=\"Exit poll versions\" />\n",
        "<figcaption aria-hidden=\"true\">Exit poll versions</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8fmoJEsHJYo"
      },
      "source": [
        "This cell will tell us how many respondents answered each version of the survey:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7MEa542HJYp"
      },
      "outputs": [],
      "source": [
        "df['VERSION'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmG7NKIkHJYu"
      },
      "source": [
        "Because each respondent answers different questions, for each row in the data, only some of the columns - the columns corresponding to questions included in that version of the survey - have data. Our classifier will need to handle that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99v6ZounHJYv"
      },
      "source": [
        "You may also notice that the data is *categorical*, not *numeric* - for each question, users choose their response from a finite set of possible answers. We will need to convert this type of data into something that our classifier can work with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAFPTSokHJYw"
      },
      "source": [
        "### Label missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNlsglGTHJYw"
      },
      "source": [
        "Since each respondent only saw a subset of questions, we expect to see missing values in each column.\n",
        "\n",
        "However, if we look at the **count** of values in each column, we see that there are no missing values - every column has the full count!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNd3pZm_HJYx"
      },
      "outputs": [],
      "source": [
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sQEu8uZHJYy"
      },
      "source": [
        "This is because missing values are recorded as a single space, and not with a NaN.\n",
        "\n",
        "Let’s change that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS97cLcHHJYy"
      },
      "outputs": [],
      "source": [
        "df.replace(\" \", float(\"NaN\"), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkEBVdMyHJYy"
      },
      "source": [
        "Now we can see an accurate count of the number of responses in each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdtih1IBHJYy"
      },
      "outputs": [],
      "source": [
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ETcktcHJYz"
      },
      "source": [
        "Notice that *every* row has some missing data! If we drop the rows with missing values, we’re left with an empty data frame (0 rows):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvjGvzWkHJYz"
      },
      "outputs": [],
      "source": [
        "df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol4Okz3tHJYz"
      },
      "source": [
        "Instead, we’ll have to make sure that the classifier we use is able to work with partial data. One nice benefit of K nearest neighbors is that it can work well with data that has missing values, as long as we use a distance metric that behaves reasonably under these conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKW-UHX6HJY0"
      },
      "source": [
        "### Encode target variable as a binary variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PEb76lmHJY0"
      },
      "source": [
        "Our goal is to classify voters based on their vote in the 2016 presidential election, i.e. the value of the `PRES` column. We will restrict our attention to the candidates from the two major parties, so we will throw out the rows representing voters who chose other candidates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlQL2OS5HJY3"
      },
      "outputs": [],
      "source": [
        "df = df[df['PRES'].isin(['Donald Trump', 'Hillary Clinton'])]\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_yMWyh-HJY3"
      },
      "outputs": [],
      "source": [
        "df['PRES'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auThOLJ8HJY4"
      },
      "source": [
        "Now, we will transform the string value into a binary variable, and save the result in `y`. We will build a binary classifier that predicts `1` if it thinks a sample is Trump voter, and `0` if it thinks a sample is a Clinton voter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmE94JEQHJY5"
      },
      "outputs": [],
      "source": [
        "y = df['PRES'].map({'Donald Trump': 1, 'Hillary Clinton': 0})\n",
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA0fnIZxHJY5"
      },
      "source": [
        "### Encode ordinal features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qqaECz5HJY5"
      },
      "source": [
        "Next, we need to encode our features. All of the features are represented as strings, but we will have to transform them into something over which we can compute a meaningful distance measure.\n",
        "\n",
        "Columns that have a **logical order** should be encoded using ordinal encoding, so that the distance metric will be meaningful.\n",
        "\n",
        "For example, consider the `AGE` column, in which users select an option from the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ0BjhccHJY8"
      },
      "outputs": [],
      "source": [
        "df['AGE'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d1_HZN9HJY9"
      },
      "source": [
        "What if we transform the `AGE` column using four binary columns: `AGE_18-29`, `AGE_30-44`, `AGE_45-65`, `AGE_65+`, with a 0 or a 1 in each column to indicate the respondent’s age?\n",
        "\n",
        "If we did this, we would lose meaningful information about the distance between ages; a respondent whose age is 18-29 would have the same distance to one whose age is 45-65 as to one whose age is 65+. Logically, we expect that a respondent whose age is 18-29 is most similar to the other 18-29 respondents, less similar to the 30-44 respondents, even less similar to the 45-65 respondents, and least similar to the 65+ respondents.\n",
        "\n",
        "To realize this, we will use **ordinal encoding**, which will represent `AGE` in a single column with *ordered* integer values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jri9CCCYHJY9"
      },
      "source": [
        "First, we define a dictionary that maps each possible value to an integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRe8ZEhzHJY-"
      },
      "outputs": [],
      "source": [
        "mapping_dict_age = {'18-29': 1,\n",
        "                 '30-44': 2,\n",
        "                 '45-65': 3,\n",
        "                 '65+': 4}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UVnXDtUHJY-"
      },
      "source": [
        "Then we can create a new data frame, `df_enc_ord`, by calling `map` on the original `df['AGE']` and passing this mapping dictionary. We will also specify that the index should be the same as the original data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Wq-6yNHJY-"
      },
      "outputs": [],
      "source": [
        "df_enc_ord = pd.DataFrame( {'AGE': df['AGE'].map( mapping_dict_age) },\n",
        "    index = df.index\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl5MmzO4HJY_"
      },
      "source": [
        "We can extend this approach to encode more than one ordinal feature. For example, let us consider the column `EDUC12R`, which includes the respondent’s answer to the question:\n",
        "\n",
        "> Which best describes your education?\n",
        ">\n",
        "> 1.  High school or less\n",
        "> 2.  Some college/assoc. degree\n",
        "> 3.  College graduate\n",
        "> 4.  Postgraduate study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "268posbeHJY_"
      },
      "outputs": [],
      "source": [
        "df['EDUC12R'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9kIpFXEHJZA"
      },
      "source": [
        "We can map both `AGE` and `EDUC12R` to ordinal-encoded columns in a new data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3qnnU4DHJZA"
      },
      "outputs": [],
      "source": [
        "mapping_dict_age = {'18-29': 1,\n",
        "                 '30-44': 2,\n",
        "                 '45-65': 3,\n",
        "                 '65+': 4}\n",
        "mapping_dict_educ12r =  {'High school or less': 1,\n",
        "                   'Some college/assoc. degree': 2,\n",
        "                   'College graduate': 3,\n",
        "                   'Postgraduate study': 4}\n",
        "df_enc_ord = pd.DataFrame( {\n",
        "    'AGE': df['AGE'].map( mapping_dict_age) ,\n",
        "    'EDUC12R': df['EDUC12R'].map( mapping_dict_educ12r)\n",
        "    },\n",
        "    index = df.index\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDHa1g48HJZB"
      },
      "source": [
        "Note that the order matters - the “High school or less” answer should have the smallest value, followed by “Some college/assoc. degree”, then “College graduate”, then “Postgraduate study”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbXShv8eHJZC"
      },
      "source": [
        "Also note that missing values are still treated as missing (not mapped to some value) - this is going to be important, since we are going to design a distance metric that treats missing values sensibly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kvv5hjPHJZC"
      },
      "outputs": [],
      "source": [
        "df_enc_ord.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxc0PYWJHJZD"
      },
      "source": [
        "There’s one more important step before we can use our ordinal-encoded values with KNN.\n",
        "\n",
        "Note that the values in the encoded columns range from 1 to the number of categories. For K nearest neighbors, the “importance” of each feature in determining the class label would be proportional to its scale (because the value of the feature is used directly in the distance metric). If we leave it as is, any feature with a larger range of possible values will be considered more “important!”, i.e. would count more in the distance metric.\n",
        "\n",
        "So, we will re-scale our encoded features to the unit interval. We can do this with the `MinMaxScaler` in `sklearn`.\n",
        "\n",
        "(Note: in general, you’d “fit” scalers etc. on only the training data, not the test data! In this case, however, the min and max in the training data is just due to our encoding, and will definitely be the same as the test data, so it doesn’t really matter.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRabESrzHJZD"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "# first scale in numpy format, then convert back to pandas df\n",
        "df_scaled = scaler.fit_transform(df_enc_ord)\n",
        "df_enc_ord = pd.DataFrame(df_scaled, columns=df_enc_ord.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsAeFE-iHJZE"
      },
      "outputs": [],
      "source": [
        "df_enc_ord.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8kzLE9iHJZE"
      },
      "outputs": [],
      "source": [
        "df_enc_ord['EDUC12R'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FpLwuaFHJZF"
      },
      "outputs": [],
      "source": [
        "df_enc_ord.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ134tFyHJZH"
      },
      "source": [
        "Later, you’ll design a model with more ordinal features. For this initial demo, though, we’ll stick to just those two - age and education - and continue to the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVb_EewzHJZH"
      },
      "source": [
        "### Encode categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foRIyenNHJZI"
      },
      "source": [
        "In the previous section, we encoded features that have a logical ordering.\n",
        "\n",
        "Other categorical features, such as `RACE`, have no logical ordering. It would be wrong to assign an ordered mapping to these features. These should be encoded using **one-hot encoding**, which will create a new column for each unique value, and then put a 1 or 0 in each column to indicate the respondent’s answer.\n",
        "\n",
        "(Note: for features that have two possible values - binary features - either categorical encoding or one-hot encoding would be valid in this case!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLbmjMXpHJZI"
      },
      "outputs": [],
      "source": [
        "df['RACE'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GqSK-8rHJZI"
      },
      "source": [
        "We can one-hot encode this column using the `get_dummies` function in `pandas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhhU8AULHJZJ"
      },
      "outputs": [],
      "source": [
        "df_enc_oh = pd.get_dummies(df['RACE'], prefix='RACE' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4aJKTmwHJZK"
      },
      "outputs": [],
      "source": [
        "df_enc_oh.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySHqEaj_HJZK"
      },
      "source": [
        "Note that we added a `RACE` prefix to each column name - this prevents overlap between columns, e.g. if we also encoded another feature where “Other” was a possible answer. And, it helps us relate the new columns back to the original survey question that they answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EHJv-U5HJZL"
      },
      "source": [
        "For this survey data, we want to preserve information about missing values - if a sample did not have a value for the `RACE` feature, we want it to have a NaN in all `RACE` columns. We can assign NaN to those rows as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6-TQQguHJZX"
      },
      "outputs": [],
      "source": [
        "df_enc_oh.loc[df['RACE'].isnull(), df_enc_oh.columns.str.startswith(\"RACE_\")] = float(\"NaN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3L0BUncHJZZ"
      },
      "source": [
        "Now, for respondents where this feature is not available, we have a NaN in all `RACE` columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY9MwCsaHJZZ"
      },
      "outputs": [],
      "source": [
        "df_enc_oh.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDvyRVkVHJZZ"
      },
      "source": [
        "### Stack columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToBnu9NKHJZa"
      },
      "source": [
        "Now, we’ll prepare our feature data, by column-wise concatenating the ordinal-encoded feature columns and the one-hot-encoded feature columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCmhvsQwHJZa"
      },
      "outputs": [],
      "source": [
        "X = pd.concat([df_enc_oh, df_enc_ord], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrAPIzSGHJZb"
      },
      "source": [
        "### Get training and test indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSil6uvEHJZb"
      },
      "source": [
        "We’ll be working with many different subsets of this dataset, including different columns.\n",
        "\n",
        "So instead of splitting up the data into training and test sets, we’ll get an array of training indices and an array of test indices using `ShuffleSplit`. Then, we can use these arrays throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoYCCLRwHJZb"
      },
      "outputs": [],
      "source": [
        "idx_tr, idx_ts = next(ShuffleSplit(n_splits = 1, test_size = 0.3, random_state = 3).split(df['PRES']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkcDt5kLHJZb"
      },
      "source": [
        "I specified the state of the random number generator for repeatability, so that every time we run this notebook we’ll have the same split. This makes it easier to discuss specific examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GySnlBstHJZc"
      },
      "source": [
        "Now, we can use the `pandas` function `.iloc` to get the training and test parts of the data set for any column.\n",
        "\n",
        "For example, if we want the training subset of `y`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwpTYEbXHJZc"
      },
      "outputs": [],
      "source": [
        "y.iloc[idx_tr]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzjYHx3iHJZc"
      },
      "source": [
        "or the test subset of `y`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSjn9xvgHJZd"
      },
      "outputs": [],
      "source": [
        "y.iloc[idx_ts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p8mLmutHJZd"
      },
      "source": [
        "Here are the summary statistics for the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJhGWbJuHJZd"
      },
      "outputs": [],
      "source": [
        "X.iloc[idx_tr].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byXqlz63HJZe"
      },
      "source": [
        "## Train a k nearest neighbors classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_huDOHSHJZe"
      },
      "source": [
        "Now that we have a target variable, a few features, and training and test indices, let’s see what happens if we try to train a K nearest neighbors classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htUx40muHJZf"
      },
      "source": [
        "### Baseline: “prediction by mode”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exps6NUMHJZf"
      },
      "source": [
        "As a baseline against which to judge the performance of our classifier, let’s find out the accuracy of a classifier that gives the majority class label (0) to all samples in our test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_GpzKJ-HJZg"
      },
      "outputs": [],
      "source": [
        "y_pred_baseline = np.repeat(0, len(y.iloc[idx_ts]))\n",
        "accuracy_score(y.iloc[idx_ts], y_pred_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78paFUQ9HJZg"
      },
      "source": [
        "A classifier trained on the data should do *at least* as well as the one that predicts the majority class label. Hopefully, we’ll be able to do much better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_t0V-_3HJZg"
      },
      "source": [
        "### `KNeighborsClassifier` does not support data with NaNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaSJGoQhHJZh"
      },
      "source": [
        "We’ve previously seen the `sklearn` implementation of a `KNeighborsClassifier`. However, that won’t work for this problem. If we try to train a `KNeighborsClassifier` on our data using the default settings, it will fail with the error message\n",
        "\n",
        "    ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
        "\n",
        "See for yourself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DZinYNoHJZh"
      },
      "outputs": [],
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "clf.fit(X.iloc[idx_tr], y.iloc[idx_tr])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hj58iYEHJZi"
      },
      "source": [
        "This is because we have many missing values in our data. And, as we explained previously, dropping rows with missing values is not a good option for this example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzLGI325HJZi"
      },
      "source": [
        "Although we cannot use the `sklearn` implementation of a `KNeighborsClassifier`, we can write our own. We need a few things:\n",
        "\n",
        "-   a function that implements a distance metric\n",
        "-   a function that accepts a distance matrix and returns the indices of the K smallest values for each row\n",
        "-   a function that returns the majority vote of the training samples represented by those indices\n",
        "\n",
        "and we have to be prepared to address complications at each stage!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcIUMaJxHJZi"
      },
      "source": [
        "### Distance metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTxmxDLAHJZj"
      },
      "source": [
        "Let’s start with the distance metric. Suppose we use an L1 distance computed over the features that are non-NaN for both samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5ujBTlEHJZk"
      },
      "outputs": [],
      "source": [
        "def custom_distance(a, b):\n",
        "  dif = np.abs(np.subtract(a,b))    # element-wise absolute difference\n",
        "  # dif will have NaN for each element where either a or b is NaN\n",
        "  l1 = np.nansum(dif, axis=1)  # sum of differences, treating NaN as 0\n",
        "  return l1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcmKE5sXHJZl"
      },
      "source": [
        "The function above expects a vector for the first argument and a matrix for the second argument, and returns a vector.\n",
        "\n",
        "For example: suppose you pass a test point $x_t$ and a matrix of training samples where each row $x_0, \\ldots, x_n$ is another training sample. It will return a vector $d_t$ with as many elements as there are training samples, and where the $i$th entry is the distance between the test point $x_t$ and training sample $x_i$.\n",
        "\n",
        "To see how to this function is used, let's consider an example with a small number of test samples and training samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io7XdFu5HJZl"
      },
      "source": [
        "Suppose we had this set of test data `a` (sampling some specific examples from the real data):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj-t6mO-HJZm"
      },
      "outputs": [],
      "source": [
        "a_idx = np.array([10296, 510,4827,20937, 22501])\n",
        "a = X.iloc[a_idx]\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHzl5PfoHJZm"
      },
      "source": [
        "and this set of training data `b`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8ES5d5bHJZo"
      },
      "outputs": [],
      "source": [
        "b_idx = np.array([10379, 4343, 7359,  1028,  2266, 131, 11833, 14106,  6682,  4402, 11899,  5877, 11758, 13163])\n",
        "b = X.iloc[b_idx]\n",
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu5y7LXxHJZq"
      },
      "source": [
        "We need to compute the distance from each sample in the test data `a`, to each sample in the training data `b`.\n",
        "\n",
        "We will set up a *distance matrix* in which to store the results. In the distance matrix, an entry in row $i$, column $j$ represents the distance between row $i$ of the test set and row $j$ of the training set.\n",
        "\n",
        "So the distance matrix should have as many rows as there are test samples, and as many columns as there are training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBXeL4qaHJZr"
      },
      "outputs": [],
      "source": [
        "distances_custom = np.zeros(shape=(len(a_idx), len(b_idx)))\n",
        "distances_custom.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "posE_zRDHJZt"
      },
      "source": [
        "Now that we have the distance matrix set up, we’re ready to fill it in with distance values. We will loop over each sample in the test set, and call the distance function passing that test sample and the entire training set.\n",
        "\n",
        "Instead of a conventional `for` loop, we will use a [tqdm](https://github.com/tqdm/tqdm) `for` loop. This library conveniently “wraps” the conventional `for` loop with a progress part, so we can see our progress while the loop is running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNXfGBrcHJZu"
      },
      "outputs": [],
      "source": [
        "# the first argument to tqdm, range(len(a_idx)), is the list we are looping over\n",
        "for idx in tqdm(range(len(a_idx)),  total=len(a_idx), desc=\"Distance matrix\"):\n",
        "  distances_custom[idx] = custom_distance(X.iloc[a_idx[idx]].values, X.iloc[b_idx].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bilyD1uhHJZu"
      },
      "source": [
        "Let’s look at those distances now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8WIckxTHJZv"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2) # show at most 2 decimal places\n",
        "print(distances_custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnuD3UlWHJZw"
      },
      "source": [
        "### Find most common class of k nearest neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1nV2kdJHJZw"
      },
      "source": [
        "Now that we have this distance matrix, for each test sample, we can:\n",
        "\n",
        "-   get an array of indices from the *distance matrix*, sorted in order of increasing distance\n",
        "-   get the list of the K nearest neighbors as the first K elements from that list,\n",
        "-   from those entries - which are indices with respect to the distance matrix - get the corresponding indices in `X` and `y`,\n",
        "-   and then predict the class of the test sample as the most common value of `y` among the nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSbEUQZnHJZx"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "# array of indices sorted in order of increasing distance\n",
        "distances_sorted = np.array([np.argsort(row) for row in distances_custom])\n",
        "# first k elements in that list = indices of k nearest neighbors\n",
        "nn_lists = distances_sorted[:, :k]\n",
        "# map indices in distance matrix back to indices in `X` and `y`\n",
        "nn_lists_idx = b_idx[nn_lists]\n",
        "# for each test sample, get the mode of `y` values for the nearest neighbors\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VZWiMFHHJZy"
      },
      "source": [
        "### Example: one test sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhvzYU-cHJZz"
      },
      "source": [
        "For example, this was the first test sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tI4ROZNHJZ0"
      },
      "outputs": [],
      "source": [
        "X.iloc[[10296]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmAV6uiSHJZ0"
      },
      "source": [
        "Here is its distance to each of the training samples in our “mini” training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPevKKeCHJZ0"
      },
      "outputs": [],
      "source": [
        "distances_custom[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXWqFkEyHJZ0"
      },
      "source": [
        "and here’s the sorted list of indices from that distance matrix - i.e. the index of the training sample with the smallest distance, the index of the training sample with the second-smallest distance, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plcaYy5MHJZ0"
      },
      "outputs": [],
      "source": [
        "distances_sorted[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZMUA9HVHJZ0"
      },
      "source": [
        "The indices (in the “mini” training sample) of the 3 nearest neighbors to this test sample are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTStqpv6HJZ0"
      },
      "outputs": [],
      "source": [
        "nn_lists[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLaTYsUpHJZ2"
      },
      "source": [
        "which corresponds to the following sample indices in the complete data `X`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQkPk8oJHJZ3"
      },
      "outputs": [],
      "source": [
        "nn_lists_idx[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf--dwymHJZ4"
      },
      "source": [
        "So, its closest neighbors in the “mini” training set are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa7qNPskHJZ4"
      },
      "outputs": [],
      "source": [
        "X.iloc[nn_lists_idx[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dadCip4HJZ4"
      },
      "source": [
        "and their corresponding values in `y` are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68TEpnmNHJZ5"
      },
      "outputs": [],
      "source": [
        "y.iloc[nn_lists_idx[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoq-_fEeHJZ5"
      },
      "source": [
        "and so the predicted label for the first test sample would be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gA8SkWZHJZ5"
      },
      "outputs": [],
      "source": [
        "y.iloc[nn_lists_idx[0]].mode().values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfpgmfAVHJZ6"
      },
      "source": [
        "### Example: entire test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8dFOB7THJZ6"
      },
      "source": [
        "Now that we understand how our custom distance function works, let’s compute the distance between every *test* sample and every *training* sample.\n",
        "\n",
        "We’ll store the results in `distances_custom`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvnfg5D8HJZ7"
      },
      "outputs": [],
      "source": [
        "distances_custom = np.zeros(shape=(len(idx_ts), len(idx_tr)))\n",
        "distances_custom.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekMJSZnyHJZ7"
      },
      "source": [
        "To compute the distance vector for each test sample, loop over the indices in the *test* set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DIdHC7BHJZ7"
      },
      "outputs": [],
      "source": [
        "for idx in tqdm(range(len(idx_ts)),  total=len(idx_ts), desc=\"Distance matrix\"):\n",
        "  distances_custom[idx] = custom_distance(X.iloc[idx_ts[idx]].values, X.iloc[idx_tr].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYecssmaHJZ8"
      },
      "source": [
        "Then, we can compute the K nearest neighbors using those distances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpXeYv2rHJZ8"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "\n",
        "# get nn indices in distance matrix\n",
        "distances_sorted = np.array([np.argsort(row) for row in distances_custom])\n",
        "nn_lists = distances_sorted[:, :k]\n",
        "\n",
        "# get nn indices in training data matrix\n",
        "nn_lists_idx = idx_tr[nn_lists]\n",
        "\n",
        "# predict using mode of nns\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTVEOZdbHJZ8"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y.iloc[idx_ts], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY9o4qslHJZ9"
      },
      "source": [
        "That is… not great."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzqQgXbCHJZ9"
      },
      "source": [
        "### Problems with our simple classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3SReGhEHJZ-"
      },
      "source": [
        "The one-sample example we saw above is enough to illustrate some basic problems with our classifier, and to explain some of the reasons for its poor performance:\n",
        "\n",
        "-   the distance metric does not really tell us how *similar* two samples are, when there are samples with missing values,\n",
        "-   and the way that ties are handled - when there are multiple samples in the training set with the same distance - is not ideal.\n",
        "\n",
        "We’ll discuss both of these, but we’ll only fix the second one in this section. Part of *your* assignment will be to address the issue with the custom distance metric in your solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb-7wrnLHJaC"
      },
      "source": [
        "In the example with the “mini” training and test sets, you may have noticed a problem: training sample 10379, which has all NaN values, has zero distance to *every* test sample according to our distance function. (Note that the first column in the distance matrix, corresponding to the first training sample, is all zeros.)\n",
        "\n",
        "This means that this sample will be a “nearest neighbor” of *every* test sample! But, it’s not necessarily *really* similar to those other test samples. We just *don’t have any information* by which to judge how similar it is to other samples. These values are *unknown*, not *similar*.\n",
        "\n",
        "The case with an all-NaN training sample is a bit extreme, but it illustrates how our simple distance metric is problematic in other situations as well. In general, when there are no missing values, for a pair of samples each feature is either *similar* or *different*. Thus a metric like L1 distance, which explicitly measures the extent to which features are *different*, also implicitly captures the extent to which features are *similar*. When samples can have missing values, though, for a pair of samples each feature is either *similar*, *different*, or *unknown* (one or both samples is missing that value). In this case, a distance metric that only measures the extent of *difference* (like L1 or L2 distance) does not capture whether the features that are not different are *similar* or *unknown*. (Our custom distance metric, which is an L1 distance, treats values that are *unknown* as if they are *similar* - neither one increases the distance.) Similarly, a distance metric that only measures the extent of *similarity* would not capture whether the features that are not similar are *different* or *unknown*.\n",
        "\n",
        "So when there are NaNs, our custom distance metric does not quite behave the way we want - we want distance between two samples to decrease with more similarity, and to increase with more differences. Our distance metric only considers difference, not similarity.\n",
        "\n",
        "For example, consider these two samples from the original data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeMvla8hHJaC"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', 150)\n",
        "disp_features = ['AGE8', 'RACE', 'REGION', 'SEX', 'SIZEPLAC', 'STANUM', 'EDUC12R', 'EDUCCOLL','INCOME16GEN', 'ISSUE16', 'QLT16', 'VERSION']\n",
        "df.iloc[[0,1889]][disp_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peYArUjpHJaD"
      },
      "source": [
        "These two samples have some things in common:\n",
        "\n",
        "-   female\n",
        "-   from suburban California\n",
        "\n",
        "but we don’t know much else about what they have in common or what they disagree on.\n",
        "\n",
        "Our distance metric will consider them very similar, because they are identical with respect to every feature that is available in both samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApDOjR5THJaD"
      },
      "outputs": [],
      "source": [
        "custom_distance(X.iloc[[0]].values, X.iloc[[1889]].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKhpv3TwHJaD"
      },
      "source": [
        "On the other hand, consider these two samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "063XuivJHJaE"
      },
      "outputs": [],
      "source": [
        "df.iloc[[0,14826]][disp_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02gM6T4KHJaE"
      },
      "source": [
        "These two samples have many more things in common:\n",
        "\n",
        "-   female\n",
        "-   Latino\n",
        "-   age 18-24\n",
        "-   no college degree\n",
        "-   income less then \\$30,000\n",
        "-   consider foreign policy to be the major issue facing the country\n",
        "-   consider “Has good judgment” to be the most important quality in deciding their presidential vote.\n",
        "\n",
        "However, they also have some differences:\n",
        "\n",
        "-   some college/associate degree vs. high school education or less\n",
        "-   suburban California vs. rural Oklahoma\n",
        "\n",
        "so the distance metric will consider them *less* similar than the previous pair, even though they have a lot in common."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf6u_tvgHJaF"
      },
      "outputs": [],
      "source": [
        "custom_distance(X.iloc[[0]].values, X.iloc[[14826]].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe2z0BtbHJaF"
      },
      "source": [
        "A better distance metric will consider the level of disagreement between samples *and* the level of agreement. That will be part of your assignment - to write a new `custom_distance`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqbq0tppHJaG"
      },
      "source": [
        "Now, let’s consider the second issue - how ties are handled.\n",
        "\n",
        "Notice that in the example with the “mini” training and test sets, for the first test sample, there was one sample with 0 distance and 3 samples with 0.33 distance. The three nearest neighbors are the sample with 0 distance, and the *first 2* of the 3 samples with 0.33 distance.\n",
        "\n",
        "In other words: ties are broken in favor of the samples that happen to have lower indices in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypGlTjuGHJaG"
      },
      "source": [
        "On a larger scale, that means that some samples will have too much influence - they will appear over and over again as nearest neighbors, just because they are earlier in the data - while some samples will not appear as nearest neighbors at all simply because of this tiebreaker behavior.\n",
        "\n",
        "If a sample is returned as a nearest neighbor very often because it happens to be closer to the test points than other points, that would be OK. But in this case, that’s not what is going on.\n",
        "\n",
        "For example, here are the nearest neighbors for the first 50 samples in the entire test set. Do you see any repetition?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CRNG2w2HJaG"
      },
      "outputs": [],
      "source": [
        "print(nn_lists_idx[0:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c5DD-iAHJaH"
      },
      "source": [
        "We find that these three samples appear very often as nearest neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmzeZ3-EHJaH"
      },
      "outputs": [],
      "source": [
        "X.iloc[[876, 10379,  1883]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfamFkw9HJaH"
      },
      "source": [
        "But other samples that have the same distance - that are actually identical in `X`! - do not appear in the nearest neighbors list at all:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ6BS0-RHJaI"
      },
      "outputs": [],
      "source": [
        "X[X['RACE_Hispanic/Latino'].eq(0) & X['RACE_Asian'].eq(0) & X['RACE_Other'].eq(0)\n",
        "  & X['RACE_Black'].eq(0) &  X['RACE_White'].eq(1)\n",
        "  & X['EDUC12R'].eq(1/3.0) & pd.isnull(X['AGE'])  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5IR0PnAHJaI"
      },
      "source": [
        "A better tiebreaker behavior would be to randomly sample from neighbors with equal distance. Fortunately, this is an easy fix:\n",
        "\n",
        "-   We had been using `argsort` to get the K smallest distances to each test point. However, if there are more than K training samples that are at the minimum distance for a particular test point (i.e. a tie of more than K values, all having the minimum distance), `argsort` will return the first K of those in order of their index in the distance matrix (their order in `idx_tr`).\n",
        "-   Now, we will use an alternative, `lexsort`, that sorts first by the second argument, then by the first argument; and we will pass a random array as the first argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KtSrZXFHJaI"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "# make a random matrix\n",
        "r_matrix = np.random.random(size=(distances_custom.shape))\n",
        "# sort using lexsort - first sort by distances_custom, then by random matrix in case of tie\n",
        "nn_lists = np.array([np.lexsort((r, row))[:k] for r, row in zip(r_matrix,distances_custom)])\n",
        "nn_lists_idx = idx_tr[nn_lists]\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKygiAE1HJaJ"
      },
      "source": [
        "Now, we don’t see nearly as much repitition of individual training samples among the nearest neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToNI4BYsHJaK"
      },
      "outputs": [],
      "source": [
        "print(nn_lists_idx[0:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eZHOMKlHJaK"
      },
      "source": [
        "Let’s get the accuracy of *this* classifier, with the better tiebreaker behavior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwYFRHjjHJaK"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y.iloc[idx_ts], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFtQbkTdHJaL"
      },
      "source": [
        "This classifier is less “fragile” - less sensitive to the draw of training data.\n",
        "\n",
        "(Depending on the random draw of training and test data, it may or may not have better performance for a particular split - but on average, across all splits of training and test data, it should be better.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42-zoXorHJaM"
      },
      "source": [
        "### Use K-fold CV to select the number of neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOupEEqcHJaM"
      },
      "source": [
        "In the previous example, we set the number of neighbors to 3, rather than letting this value be dictated by the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpoFGoIuHJaN"
      },
      "source": [
        "As a next step, to improve the classifier performance, we can use K-fold CV to select the number of neighbors. Note that depending how we do it, this can be *very* computationally expensive, or it can be not much more computationally expensive than just fixing the number of neighbors ourselves.\n",
        "\n",
        "The most expensive part of the algorithm is computing the distance to the training samples. This is $O(nd)$ for each test sample, where $n$ is the number of training samples and $d$ is the number of features. If we can make sure this computation happens only once, instead of once per fold, this process will be fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmlCjLyOHJaN"
      },
      "source": [
        "Here, we pre-compute our distance matrix for *every* training sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6fAqSeXHJaO"
      },
      "outputs": [],
      "source": [
        "# pre-compute a distance matrix of training vs. training data\n",
        "distances_kfold = np.zeros(shape=(len(idx_tr), len(idx_tr)))\n",
        "\n",
        "for idx in tqdm(range(len(idx_tr)),  total=len(idx_tr), desc=\"Distance matrix\"):\n",
        "  distances_kfold[idx] = custom_distance(X.iloc[idx_tr[idx]].values, X.iloc[idx_tr].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMj4m92nHJaO"
      },
      "source": [
        "Now, we’ll use K-fold CV.\n",
        "\n",
        "In each fold, as always, we’ll further divide the training data into validation and training sets.\n",
        "\n",
        "Then, we’ll select the *rows* of the pre-computed distance matrix corresponding to the *validation* data in this fold, and the *columns* of the pre-computed distance matrix corresponding to the *training* data in this fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jW1_rq6HJaO"
      },
      "outputs": [],
      "source": [
        "n_fold = 5\n",
        "k_list = np.arange(1, 301, 10)\n",
        "n_k = len(k_list)\n",
        "acc_list = np.zeros((n_k, n_fold))\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "for isplit, idx_k in enumerate(kf.split(idx_tr)):\n",
        "\n",
        "  print(\"Iteration %d\" % isplit)\n",
        "\n",
        "  # Outer loop: select training vs. validation data (out of training data!)\n",
        "  idx_tr_k, idx_val_k = idx_k\n",
        "\n",
        "  # get target variable values for validation data\n",
        "  y_val_kfold = y.iloc[idx_tr[idx_val_k]]\n",
        "\n",
        "  # get distance matrix for validation set vs. training set\n",
        "  distances_val_kfold   = distances_kfold[idx_val_k[:, None], idx_tr_k]\n",
        "\n",
        "  # generate a random matrix for tie breaking\n",
        "  r_matrix = np.random.random(size=(distances_val_kfold.shape))\n",
        "\n",
        "  # loop over the rows of the distance matrix and the random matrix together with zip\n",
        "  # for each pair of rows, return sorted indices from distances_val_kfold\n",
        "  distances_sorted = np.array([np.lexsort((r, row)) for r, row in zip(r_matrix,distances_val_kfold)])\n",
        "\n",
        "  # Inner loop: select value of K, number of neighbors\n",
        "  for idx_k, k in enumerate(k_list):\n",
        "\n",
        "    # now we select the indices of the K smallest, for different values of K\n",
        "    # the indices in  distances_sorted are with respect to distances_val_kfold\n",
        "    # from those - get indices in idx_tr_k, then in X\n",
        "    nn_lists_idx = idx_tr[idx_tr_k[distances_sorted[:,:k]]]\n",
        "\n",
        "    # get validation accuracy for this value of k\n",
        "    y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]\n",
        "    acc_list[idx_k, isplit] = accuracy_score(y_val_kfold, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl0eaJMqHJaP"
      },
      "source": [
        "Here’s how the validation accuracy changes with number of neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_apoHVnHJaP"
      },
      "outputs": [],
      "source": [
        "plt.errorbar(x=k_list, y=acc_list.mean(axis=1), yerr=acc_list.std(axis=1)/np.sqrt(n_fold-1));\n",
        "\n",
        "plt.xlabel(\"k (number of neighbors)\");\n",
        "plt.ylabel(\"K-fold accuracy\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvATXiEUHJaQ"
      },
      "source": [
        "Using this, we can find a better choice for k (number of neighbors):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgkC9dUIHJaQ"
      },
      "outputs": [],
      "source": [
        "best_k = k_list[np.argmax(acc_list.mean(axis=1))]\n",
        "print(best_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TJTtHteHJaQ"
      },
      "source": [
        "Now, let’s re-run our KNN algorithm using the entire training set and this `best_k` number of neighbors, and check its accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBIyRlpMHJaQ"
      },
      "outputs": [],
      "source": [
        "r_matrix = np.random.random(size=(distances_custom.shape))\n",
        "nn_lists = np.array([np.lexsort((r, row))[:best_k] for r, row in zip(r_matrix,distances_custom)])\n",
        "nn_lists_idx = idx_tr[nn_lists]\n",
        "y_pred =  [y.iloc[nn].mode()[0] for nn in nn_lists_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIUs1etqHJaR"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y.iloc[idx_ts], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrs3Oo-mHJaR"
      },
      "source": [
        "### Summarizing our basic classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYLbijyVHJaR"
      },
      "source": [
        "Our basic classifier:\n",
        "\n",
        "-   uses three features (age, race, and education) to predict a respondent’s vote\n",
        "-   doesn’t mind if there are NaNs in the data (unlike the `sklearn` implementation, which throws an error)\n",
        "-   uses a random tiebreaker if there are multiple training samples with the same distance to the test sample\n",
        "-   uses the number of neighbors with the best validation accuracy, according to K-fold CV.\n",
        "\n",
        "But, there are some outstanding issues:\n",
        "\n",
        "-   we have only used three features, out of many more available features.\n",
        "-   the distance metric only cares about the degree of disagreement (difference) between two samples, and doesn’t balance it against the degree of agreement (similarity).\n",
        "\n",
        "For this assignment, you will create an even better classifier by improving on those two issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OgKj_5_HJaR"
      },
      "source": [
        "## Create a better classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVYHtqE9HJaR"
      },
      "source": [
        "In the remaining sections of this notebook, you’ll need to fill in code to:\n",
        "\n",
        "-   implement a custom distance metric\n",
        "-   encode more features\n",
        "-   implement feature selection or feature weighting\n",
        "-   “train” and evaluate your final classifier, including K-Fold CV to select the best value for number of neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOYvb6caHJaS"
      },
      "source": [
        "### Create a better distance metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSnJ_kqVHJaS"
      },
      "source": [
        "Your first task is to improve on the basic distance metric we used above. There is no one correct answer - there are many ways to compute a distance - but for full credit, your distance metric should satisfy the following criteria:\n",
        "\n",
        "1.  if two samples are identical, the distance between them should be zero.\n",
        "2.  as the extent of *difference* between two samples increases, the distance should increase.\n",
        "3.  as the extent of *similarity* between two samples increases, the distance should decrease.\n",
        "4.  if in a pair of samples one or both have a NaN value for a given feature, the similarity or difference of this feature is *unknown*. Your distance metric should compute a smaller distance for a pair of samples with many similarities (even if there is some small difference) than for a pair of samples with mostly unknown similarity.\n",
        "\n",
        "You should also avoid explicit `for` loops inside the `custom_distance` function - use efficient `numpy` functions instead. Note that `numpy` includes many functions that are helpful when working with arrays that have NaN values, including mathematical functions like [sum](https://numpy.org/doc/stable/reference/generated/numpy.nansum.html), [product](https://numpy.org/doc/stable/reference/generated/numpy.nanprod.html), [max](https://numpy.org/doc/stable/reference/generated/numpy.nanmax.html) and [min](https://numpy.org/doc/stable/reference/generated/numpy.nanmin.html), and logic functions like [isnan](https://numpy.org/doc/stable/reference/generated/numpy.isnan.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3DUq3WfHJaT"
      },
      "source": [
        "#### Implement your distance metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVjZa7dVHJaT"
      },
      "outputs": [],
      "source": [
        "# TODO - implement distance metric\n",
        "\n",
        "def custom_distance(a, b):\n",
        "  # fill in your solution here!\n",
        "  # you are encouraged to use efficient numpy functions where possible\n",
        "  # refer to numpy documentation\n",
        "\n",
        "  # this is just a placeholder - your function shouldn't actually return\n",
        "  # all zeros ;)\n",
        "  return np.zeros(b.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rlscoOHHJaT"
      },
      "source": [
        "#### Test cases for your distance metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXoI60iHHJaU"
      },
      "source": [
        "You can use these test samples to check your work. (But, your metric should also satisfy the criteria in general - not only for these specific cases!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffjaGLzzHJaU"
      },
      "source": [
        "First criteria: if two samples are identical, the distance between them should be zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a85cYNv0HJaV"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, 1, 0,      1, 0, 0.3]] )  # A0 - test sample\n",
        "b = np.array([[0, 1, 0,      1, 0, 0.3]] )  # B0 - same as A0, should have 0 distance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPFLxW9ZHJaV"
      },
      "outputs": [],
      "source": [
        "distances_ex = np.zeros(shape=(len(a), len(b)))\n",
        "for idx, a_i in enumerate(a):\n",
        "  distances_ex[idx] = custom_distance(a_i, b)\n",
        "\n",
        "print(distances_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc0HimM8HJaW"
      },
      "source": [
        "Second criteria: as the extent of *difference* between two samples increases, the distance should increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY1UE3YAHJab"
      },
      "source": [
        "These should have *increasing* distance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iguf3869HJae"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, 1, 0,      1, 0, 0.3]] )  # A0 - test sample\n",
        "b = np.array([[0, 1, 0,      1, 0, 0.3],              # B0 - same as A0, should have 0 distance\n",
        "              [0, 1, 0,      1, 0, 0.5],              # B1 - has one small difference, should have larger distance than B0\n",
        "              [0, 1, 0,      1, 0, 1  ],              # B2 - has more difference, should have larger distance than B1\n",
        "              [0, 0, 0,      1, 0, 0  ],              # B3 - has even more difference\n",
        "              [1, 0, 1,      0, 1, 0  ]])             # B4 - has the most difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P_Q4EUKHJag"
      },
      "outputs": [],
      "source": [
        "distances_ex = np.zeros(shape=(len(a), len(b)))\n",
        "for idx, a_i in enumerate(a):\n",
        "  distances_ex[idx] = custom_distance(a_i, b)\n",
        "\n",
        "print(distances_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfkVYI-eHJah"
      },
      "source": [
        "These should have *decreasing* distance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV9BQ2KbHJai"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, 1, 0, 1, 0, 1]] )            # A0 - test sample\n",
        "b = np.array([[1, 0, 1, 0, 1, 0],              # B0 - completely different, should have large distance\n",
        "              [1, 0, 1, 0, 1, np.nan],         # B1 - less difference than B0, should have less distance\n",
        "              [1, 0, 1, 0, np.nan, np.nan]])   # B2 - even less difference than B1, should have less distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45iQ2AUlHJai"
      },
      "outputs": [],
      "source": [
        "distances_ex = np.zeros(shape=(len(a), len(b)))\n",
        "for idx, a_i in enumerate(a):\n",
        "  distances_ex[idx] = custom_distance(a_i, b)\n",
        "\n",
        "print(distances_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9aySCMzHJak"
      },
      "source": [
        "Third criteria: as the extent of *similarity* between two samples increases, the distance should decrease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3Ik3jNrHJam"
      },
      "source": [
        "These should have *increasing* distance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcoeaDEmHJan"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, 1, 0, 1, 0, 0.3]] )  # A0 - test sample\n",
        "b = np.array([[0, 1, 0, 1, 0, 0.3],              # B0 - same as A0, should have 0 distance\n",
        "              [0, 1, 0, 1, 0, np.nan],           # B1 - has less similarity than B0, should have larger distance\n",
        "              [0, 1, 0, 1, np.nan, np.nan],      # B2 - has even less similarity, should have larger distance\n",
        "              [0, np.nan, np.nan, np.nan, np.nan, np.nan]])     # B3 - has least similarity, should have larger distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_Unr5N-HJaq"
      },
      "outputs": [],
      "source": [
        "distances_ex = np.zeros(shape=(len(a), len(b)))\n",
        "for idx, a_i in enumerate(a):\n",
        "  distances_ex[idx] = custom_distance(a_i, b)\n",
        "\n",
        "print(distances_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7C3Zm8HJax"
      },
      "source": [
        "Fourth criteria: if in a pair of samples one or both have a NaN value for a given feature, the similarity or difference of this feature is *unknown*. Your distance metric should compute a smaller distance for a pair of samples with many similarities (even if there is some small difference) than for a pair of samples with mostly unknown similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghOqe30bHJay"
      },
      "source": [
        "These should have *increasing* distance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktfEKFIyHJa0"
      },
      "outputs": [],
      "source": [
        "a = np.array([[0, np.nan, 0, 1, np.nan, 0.3]] )  # A0 - test sample\n",
        "b = np.array([[0, np.nan, 0, 1, 0,      0.5],                # B0 - three similar features, one small difference\n",
        "              [0, np.nan, np.nan, np.nan, np.nan, np.nan]])  # B1 - much less similarity than B0, should have larger distance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMLs0I5DHJa3"
      },
      "outputs": [],
      "source": [
        "distances_ex = np.zeros(shape=(len(a), len(b)))\n",
        "for idx, a_i in enumerate(a):\n",
        "  distances_ex[idx] = custom_distance(a_i, b)\n",
        "\n",
        "print(distances_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz8070FqHJa4"
      },
      "source": [
        "### Encode more features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZTsXDlGHJa6"
      },
      "source": [
        "Our basic classifier used three features: age, race, and education. But there are many more features in this data that may be predictive of vote:\n",
        "\n",
        "-   More demographic information: `INCOME16GEN`, `MARRIED`, `RELIGN10`, `ATTEND16`, `LGBT`, `VETVOTER`, `SEX`\n",
        "-   Opinions about political issues and about what factors are most important in determining which candidate to vote for: `TRACK`, `SUPREME16`, `FINSIT`, `IMMWALL`, `ISIS16`, `LIFE`, `TRADE16`, `HEALTHCARE16`, `GOVTDO10`, `GOVTANGR16`, `QLT16`, `ISSUE16`, `NEC`\n",
        "\n",
        "in addition to the features `AGE`, `RACE`, and `EDUC12R`.\n",
        "\n",
        "You will try to improve the model by adding some of these features.\n",
        "\n",
        "(Note that we will *not* use questions that directly ask the participants how they feel about individual candidates, or about their party affiliation or political leaning. These features are a close proxy for the target variable, and we’re going to assume that these are not available to the model.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLqiWNx4HJa-"
      },
      "source": [
        "Refer to the PDF documentation to see the question and the possible answers corresponding to each of these features. You may also choose to do some exploratory data analysis, to help you understand these features better.\n",
        "\n",
        "For your convenience, here are all the possible answers to those survey questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_dxTCzoHJbE"
      },
      "outputs": [],
      "source": [
        "features = ['INCOME16GEN', 'MARRIED', 'RELIGN10', 'ATTEND16', 'LGBT', 'VETVOTER',\n",
        "            'SEX', 'TRACK', 'SUPREME16',  'FINSIT', 'IMMWALL', 'ISIS16', 'LIFE',\n",
        "            'TRADE16', 'HEALTHCARE16', 'GOVTDO10', 'GOVTANGR16', 'QLT16',\n",
        "            'ISSUE16', 'NEC']\n",
        "\n",
        "for f in features:\n",
        "  print(f)\n",
        "  print(df[f].value_counts())\n",
        "  print(\"***************************************************\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyW0-p77HJbF"
      },
      "source": [
        "It is up to you to decide which features to include in your model. However, you must encode at least eight features, including:\n",
        "\n",
        "-   at least four features that are encoded using an ordinal encoder because they have a logical order (and you should include an explicit mapping for these), and\n",
        "-   at least four features that are encoded using one-hot encoding because they have no logical order.\n",
        "\n",
        "Binary features - features that can take on only two values - “count” toward either category.\n",
        "\n",
        "(If you decide to use the features I used above, they do “count” as part of the four. For example, you could use age, education, and two additional ordinal-encoded features, and race and three other one-hot-encoded features.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uul2fARUHJbF"
      },
      "source": [
        "#### Encode ordinal features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra6jeTdhHJbG"
      },
      "source": [
        "In the following cells, prepare your ordinal encoded features as demonstrated in the “Prepare data \\> Encode ordinal features” section earlier in this notebook.\n",
        "\n",
        "Use at least four features that are encoded using an ordinal encoder. (You can choose which features to include, but they should be either binary features, or features for which the values have a logical ordering that should be preserved in the distance computations!)\n",
        "\n",
        "Also:\n",
        "\n",
        "-   Save the ordinal-encoded columnns in a data frame called `df_enc_ord`.\n",
        "-   You should explicitly specify the mappings for these, so that you can be sure that they are encoded using the correct logical order.\n",
        "-   For some questions, there is also an “Omit” answer - if a respondent left that question blank on the questionnaire, the value for that question will be “Omit”. Since “Omit” has no logical place in the order, we’re going to treat these as missing values: don’t include “Omit” in your `mapping_ord` dictionary, and then these Omit values will be encoded as NaN.\n",
        "-   Make sure to scale each column to the range 0-1, as demonstrated in the “Prepare data \\> Encode ordinal features” section earlier in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLfq_9SFHJbG"
      },
      "outputs": [],
      "source": [
        "# TODO - encode ordinal features\n",
        "\n",
        "# set up mapping dictionary and list of features to encode with ordinal encoding\n",
        "\n",
        "# use map to get the encoded columns, save in df_enc_ord\n",
        "df_enc_ord = ...\n",
        "\n",
        "# scale each column to the range 0-1\n",
        "df_enc_ord =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6s9H2WeHJbH"
      },
      "source": [
        "Look at the encoded data to check your work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRBZS9ezHJbI"
      },
      "outputs": [],
      "source": [
        "df_enc_ord.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3cSJDTlHJbI"
      },
      "source": [
        "#### Encode categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLlPuvQlHJbJ"
      },
      "source": [
        "In the following cells, prepare your categorical encoded features as demonstrated in the “Prepare data \\> Encode categorical features” section earlier in this notebook.\n",
        "\n",
        "Use at least four features that are encoded using an categorical encoder. (You can choose which features to include, but they should be either binary features, or features for which the values do *not* have a logical ordering that should be preserved in the distance computations!)\n",
        "\n",
        "Also:\n",
        "\n",
        "-   Save the categorical-encoded columnns in a data frame called `df_enc_oh`.\n",
        "-   For some questions, there is also an “Omit” answer - if a respondent left that question blank on the questionnaire, the value for that question will be “Omit”. We’re going to treat these as missing values. Before encoding the NaN values, you should drop the column corresponding to the “Omit” value from the data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga5BsKzpHJbK"
      },
      "outputs": [],
      "source": [
        "# TODO - encode categorical features\n",
        "\n",
        "\n",
        "# use get_dummies to get the encoded columns, stack and save in df_enc_oh\n",
        "df_enc_oh = ...\n",
        "\n",
        "# drop the Omit columns, if any of these are in the data frame\n",
        "df_enc_oh.drop(['ISSUE16_Omit', 'QLT16_Omit', 'TRACK_Omit','IMMWALL_Omit','GOVTDO10_Omit'],\n",
        "                axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "# if a respondent did not answer a question, make sure they have NaN in all the columns corresonding to that question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc8F38mrHJbK"
      },
      "source": [
        "#### Stack columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orV1uKCYHJbL"
      },
      "source": [
        "Now, we’ll create a combined data frame with all of the encoded features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzAlogLEHJbM"
      },
      "outputs": [],
      "source": [
        "X = pd.concat([df_enc_oh, df_enc_ord], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpW1EfzLHJbN"
      },
      "outputs": [],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeZVzIhwHJbQ"
      },
      "source": [
        "### Feature selection or feature weighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uv6A9z0HJbT"
      },
      "source": [
        "Because the K nearest neighbor classifier weights each feature equally in the distance metric, including features that are not relevant for predicting the target variable can actually make performance worse.\n",
        "\n",
        "To improve performance, you could either:\n",
        "\n",
        "-   use a subset of features that are most important, or\n",
        "-   use feature weights, so that more important features are scaled up and less important features are scaled down.\n",
        "\n",
        "Feature selection has another added benefit - if you use fewer features, than you also get a faster inference time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwqiE7foHJbU"
      },
      "source": [
        "There are many options for feature selection or feature weighting, and you can choose anything that seems reasonable to you - there isn’t one right answer here! But, you will have to explain and justify your choice. In our lesson on feature selection/weighting, we discussed two parts to the problem of identifying the best subset of features:\n",
        "\n",
        "-   **Search**: you will have to describe the search strategy you use to determine the features or feature subsets to evaluate.\n",
        "-   **Evaluate**: you will have to describe the approach you use to evaluate the “goodness” of a feature or feature subset. Since this dataset has the added complication of missing values, you should also make sure to explain how you handle missing values in your evaluation.\n",
        "\n",
        "And, you will have to describe the approach you used to select the best **number** of features to include or best **size** of feature subset (if you are using feature selection, not feature weighting).\n",
        "\n",
        "For full credit, you will have to convince me that the approach you selected is a good match for (1) the data, and (2) the learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zBIkww8HJbU"
      },
      "source": [
        "In the following cell, implement feature selection or feature weighting, and return the results in `X_trans`:\n",
        "\n",
        "-   If you use feature selection, `X_trans` should have all of the rows of `X`, but only a subset of its columns. You should create a variable `feat_inc` which is a list of all of the features you want to include in the model.\n",
        "-   If you use feature weighting, `X_trans` should have the same dimensions of `X`, but instead of each column being in the range 0-1, each column will be scaled according to its importance (more important features will be scaled up, less important features will be scaled down). You should create a variable `feat_wt` which has a weight for every feature in `X`. Then, you’ll multiply `X` by `feat_wt` to get `X_trans`.\n",
        "\n",
        "Some important notes:\n",
        "\n",
        "-   The goal is to write code to find the feature selection or feature weighting, not to find it by manual inspection! Don’t hard-code any values.\n",
        "-   Although `X_trans` will include all rows of the data, you should not use the test data in the process of finding `feat_inc` or `feat_wt`! Feature selection and feature weighting are considered part of model fitting, and so only the training data may be used in this process.\n",
        "-   For the “search” part of the optimization, you should not use any `sklearn` function or equivalent from another library - write pure Python+numpy code to implement the search yourself. For the “evaluate” part of the optimization, you are free to use an `sklearn` function, but make sure you understand what it does and are sure it is a good fit for the data and the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx6R-0dNHJbV"
      },
      "outputs": [],
      "source": [
        "# TODO - feature selection OR feature weighting\n",
        "\n",
        "# if you choose feature selection\n",
        "# feat_inc = ...\n",
        "# X_trans = X[feat_inc]\n",
        "\n",
        "# if you choose feature weighting\n",
        "# feat_wt =\n",
        "# X_trans = X.multiply(feat_wt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9k2G_zDHJbX"
      },
      "source": [
        "Check your work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhndb9T1HJbX"
      },
      "outputs": [],
      "source": [
        "X_trans.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFBwTN1rHJbY"
      },
      "source": [
        "#### TODO - describe your approach to feature selection or feature weighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5RMe8aRHJbY"
      },
      "source": [
        "In a text cell, describe **in detail** the approach you used for feature selection or feature weighting. Your answer should include the following parts, in paragraph form:\n",
        "\n",
        "-   **Part 1: Search**: describe the search strategy you use to determine the features or feature subsets to evaluate. Is the approach you chose guaranteed to evaluate the optimal feature subset? How many feature subsets do you need to consider as part of your approach?\n",
        "-   **Part 2: Evaluate**: describe the approach you use to evaluate the “goodness” of a feature or feature subset. Did you use a filter method or a wrapper method? What was the scoring function or model you used to evaluate the “goodness” of a feature or feature subset, and why? And since this dataset has the added complication of missing values, you should also make sure to explain how you handle missing values in your evaluation.\n",
        "-   **Part 3: Number/size**: if you are using feature selection, not feature weighting: Describe the approach you used to select the best **number** of features to include or best **size** of feature subset.\n",
        "\n",
        "Also explain: Why is the approach you chose well suited for *this data* and *this model*? And, what are some disadvantages or limitations of the approach you chose?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFjmY-WiHJbZ"
      },
      "source": [
        "### Evaluate final classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxoij7jbHJbk"
      },
      "source": [
        "Finally, you’ll repeat the process of finding the best number of neighbors using K-fold CV, with your “transformed” data (`X_trans`) and your new custom distance metric.\n",
        "\n",
        "Then, you’ll evaluate the performance of your model on the *test* data, using that optimal number of neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwG0lyQ5HJbl"
      },
      "outputs": [],
      "source": [
        "# TODO - evaluate - pre-compute distance matrix of training vs. training data\n",
        "\n",
        "distances_kfold = ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CZjiaSTHJbm"
      },
      "outputs": [],
      "source": [
        "# TODO - evaluate - use K-fold CV, fill in acc_list\n",
        "\n",
        "n_fold = 5\n",
        "k_list = np.arange(1, 301, 10)\n",
        "n_k = len(k_list)\n",
        "acc_list = np.zeros((n_k, n_fold))\n",
        "\n",
        "# use this random state so your results will match the auto-graders'\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=3)\n",
        "\n",
        "for isplit, idx_k in enumerate(kf.split(idx_tr)):\n",
        "\n",
        "  # Outer loop\n",
        "\n",
        "  for idx_k, k in enumerate(k_list):\n",
        "\n",
        "    # Inner loop\n",
        "\n",
        "    acc_list[idx_k, isplit] = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeYqp8qtHJbo"
      },
      "source": [
        "See how the validation accuracy changes with number of neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_edhpsqxHJbp"
      },
      "outputs": [],
      "source": [
        "plt.errorbar(x=k_list, y=acc_list.mean(axis=1), yerr=acc_list.std(axis=1)/np.sqrt(n_fold-1));\n",
        "\n",
        "plt.xlabel(\"k (number of neighbors)\");\n",
        "plt.ylabel(\"K-fold accuracy\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJopERLiHJbp"
      },
      "source": [
        "Find the best choice for k (number of neighbors) using the “highest validation accuracy” rule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDYWf5iiHJbp"
      },
      "outputs": [],
      "source": [
        "# TODO - evaluate - find best k\n",
        "best_k = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRr9w-VQHJbq"
      },
      "source": [
        "Finally, re-run our KNN algorithm using the entire training set and this `best_k` number of neighbors. Check its accuracy on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtzfhNZKHJbq"
      },
      "outputs": [],
      "source": [
        "# TODO - evaluate - find accuracy\n",
        "# compute distance matrix for test vs. training data\n",
        "# use KNN with best_k to find y_pred for test data\n",
        "y_pred = ...\n",
        "# compute accuracy\n",
        "acc = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5mQ6EVKHJbr"
      },
      "outputs": [],
      "source": [
        "print(acc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}